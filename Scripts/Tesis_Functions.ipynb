{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#pip install unidecode"
      ],
      "metadata": {
        "id": "PirKvJ8KLcTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression, f_classif\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import warnings\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, r2_score, confusion_matrix\n",
        "from sklearn.utils import check_array\n",
        "from unidecode import unidecode\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n"
      ],
      "metadata": {
        "id": "AMVVyKt9Fsfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def balancear_datos(X, Y):\n",
        "    # Instancia el RandomOverSampler\n",
        "    ros = RandomOverSampler(random_state=13)\n",
        "\n",
        "    # Aplica la técnica de sobremuestreo para balancear las clases\n",
        "    X_resampled, Y_resampled = ros.fit_resample(X, Y)\n",
        "\n",
        "    return X_resampled, Y_resampled\n",
        "print('balancear_datos Loaded')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1_FVaz1frvs",
        "outputId": "f14c1a70-94e8-4271-e02a-0c9540d1fb87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "balancear_datos Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Qovp-AeFSMo",
        "outputId": "f345b4c3-5d7d-496b-d9a3-af1deaec3ce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selectkbest_features Loaded\n"
          ]
        }
      ],
      "source": [
        "# SelectKBest : Selecciona las variables Relevantes Utilizando SelectKBest\n",
        "\n",
        "def Selectkbest_features(X, y, k,tipo_modelo=f_regression):\n",
        "    selector = SelectKBest(score_func=f_regression, k=k)  # Utilizamos f_regression como función de puntuación\n",
        "    X_new = selector.fit_transform(X, y)  # Seleccionamos las k mejores características\n",
        "\n",
        "    mask = selector.get_support()  # Máscara booleana de las características seleccionadas\n",
        "    selected_features = [feature for feature, mask_value in zip(X.columns, mask) if mask_value]\n",
        "    selected_scores = selector.scores_[selector.get_support()]\n",
        "\n",
        "    # Crear un DataFrame con las variables seleccionadas y sus puntuaciones\n",
        "    selected_data = pd.DataFrame({'Variable': selected_features, 'Puntuacion': selected_scores})\n",
        "\n",
        "#f_regression; f_classif\n",
        "    return selected_data\n",
        "print('Selectkbest_features Loaded')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RFE: Fucnion que obtiene las variables utilizanfo el modelo de Recursive Feature Elimination.\n",
        "\n",
        "def rfe_features(estimator, X, y, n_features_to_select=None):\n",
        "\n",
        "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "    selector = RFE(estimator, n_features_to_select=n_features_to_select)\n",
        "    selector.fit(X, y)\n",
        "\n",
        "    # Obtener las variables seleccionadas\n",
        "    selected_features = X.columns[selector.support_].tolist()\n",
        "\n",
        "\n",
        "    # Obtener los puntajes de las características seleccionadas\n",
        "    feature_scores = selector.estimator_.coef_\n",
        "\n",
        "    # Crear el DataFrame de resultado\n",
        "    df_result = pd.DataFrame({'Variable': selected_features, 'Score': feature_scores})\n",
        "    warnings.filterwarnings(\"default\", category=DeprecationWarning)\n",
        "    return df_result\n",
        "print('rfe_features Loaded')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gox4ZVmcFnjD",
        "outputId": "1bcd5993-5bfc-42e8-e70d-2fd8808d6fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rfe_features Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Integración de Variables\n",
        "\n",
        "def integrate_features(Selectkbest_List,Boruta_List,Rfe_List):\n",
        "\n",
        "    integrated_features = list(set(Selectkbest_List + Boruta_List + Rfe_List))\n",
        "\n",
        "    return integrated_features\n",
        "print('Integración de Variables Loaded')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APIeOedPFngB",
        "outputId": "06853120-c3eb-4738-b4a5-40d1cefb8f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integración de Variables Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Funcion de evaluacion de los modelos\n",
        "\n",
        "def evaluate_results(y_true, y_pred):\n",
        "    # Curva ROC\n",
        "    roc_auc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "    # Error cuadrático medio (MSE)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    # Raíz del error cuadrático medio (RMSE)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # Coeficiente de determinación (R2)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # Error porcentual absoluto medio (MAPE)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "    return roc_auc, mse, rmse, r2, mape\n",
        "print('evaluacion de los modelos Loaded')"
      ],
      "metadata": {
        "id": "ewUJSBhTAeLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generacion de Componentes Principales\n",
        "\n",
        "def generar_componentes_principales_X(dataframe, n_componentes):\n",
        "    # Inicializar el objeto PCA con el número de componentes deseados\n",
        "    pca = PCA(n_components=n_componentes)\n",
        "\n",
        "    # Realizar el PCA y transformar los datos\n",
        "    componentes_principales = pca.fit_transform(dataframe)\n",
        "\n",
        "    # Crear un DataFrame con los componentes principales\n",
        "    df_componentes = pd.DataFrame(data=componentes_principales, columns=[f'Componente_{i+1}' for i in range(n_componentes)])\n",
        "\n",
        "    return df_componentes\n",
        "print('Pca_components Loaded')"
      ],
      "metadata": {
        "id": "NQ9RCVb3OyAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a9df94-9984-4c74-a755-a6d475aa41a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pca_components Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Funcion que encuentra las variables utilziando tensorflow\n",
        "\n",
        "def find_important_variables(X, y, model_type='random_forest', n_estimators=100):\n",
        "    # Configurar TensorFlow para utilizar la GPU\n",
        "    physical_devices = tf.config.list_physical_devices('GPU')\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "    if model_type == 'random_forest':\n",
        "        # Crear el modelo Random Forest\n",
        "        model = RandomForestRegressor(n_estimators=n_estimators)\n",
        "\n",
        "    elif model_type == 'xgboost':\n",
        "        # Crear el modelo XGBoost\n",
        "        model = XGBRegressor(n_estimators=n_estimators)\n",
        "\n",
        "    else:\n",
        "        raise ValueError('Model type not supported.')\n",
        "\n",
        "    # Convertir los datos a tensores de TensorFlow\n",
        "    X_tensor = tf.convert_to_tensor(X.values, dtype=tf.float32)\n",
        "    y_tensor = tf.convert_to_tensor(y.values, dtype=tf.float32)\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    with tf.device('/GPU:0'):\n",
        "        model.fit(X_tensor, y_tensor)\n",
        "\n",
        "    # Obtener la importancia de las variables\n",
        "    if model_type == 'random_forest':\n",
        "        feature_importances = model.feature_importances_\n",
        "    elif model_type == 'xgboost':\n",
        "        feature_importances = model.feature_importances_\n",
        "    else:\n",
        "        feature_importances = None\n",
        "\n",
        "    if feature_importances is not None:\n",
        "        # Obtener los nombres de las variables\n",
        "        variable_names = X.columns\n",
        "\n",
        "        # Crear un DataFrame con las variables y su importancia\n",
        "        importance_df = pd.DataFrame({'Variable': variable_names, 'Importance': feature_importances})\n",
        "\n",
        "        # Ordenar las variables por su importancia en orden descendente\n",
        "        importance_df = importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "        return importance_df\n",
        "    else:\n",
        "        raise ValueError('Feature importances not available.')\n",
        "print('Improtancia con XGBoost y Random forest Loaded')"
      ],
      "metadata": {
        "id": "N8YvHNoSAbNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modelo para encontrar variables importantes usando XGBOOST y RANDOM FOREST\n",
        "def find_important_variables_Clasificacion(X, y, model_type='xgboost', n_estimators=100):\n",
        "    if model_type == 'xgboost':\n",
        "        # Crear el modelo XGBoost clasificador\n",
        "        model = xgb.XGBClassifier(n_estimators=n_estimators)\n",
        "\n",
        "    elif model_type == 'random_forest':\n",
        "        # Crear el modelo Random Forest clasificador\n",
        "        model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
        "\n",
        "    else:\n",
        "        raise ValueError('Model type not supported. Use \"xgboost\" or \"random_forest\".')\n",
        "\n",
        "\n",
        "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Obtener la importancia de las variables\n",
        "    feature_importances = model.feature_importances_\n",
        "\n",
        "    # Obtener los nombres de las variables\n",
        "    variable_names = X.columns\n",
        "\n",
        "    # Crear un DataFrame con las variables y su importancia\n",
        "    importance_df = pd.DataFrame({'Variable': variable_names, 'Importance': feature_importances})\n",
        "\n",
        "    # Ordenar las variables por su importancia en orden descendente\n",
        "    importance_df = importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    return importance_df"
      ],
      "metadata": {
        "id": "FZocdMbH84l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xgboost_model_evaluation(X_train, X_test, y_train, y_test,param_grid):\n",
        "\n",
        "    # Crear el modelo XGBoost\n",
        "    xgb_model = XGBRegressor()\n",
        "\n",
        "    # Realizar Grid Search con validación cruzada\n",
        "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Obtener los mejores hiperparámetros\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Crear el modelo XGBoost con los mejores hiperparámetros\n",
        "    xgb_best_model = XGBRegressor(**best_params)\n",
        "\n",
        "    # Ajustar el modelo con los mejores hiperparámetros a los datos de entrenamiento\n",
        "    xgb_best_model.fit(X_train, y_train)\n",
        "\n",
        "    # Realizar la predicción en el grupo de prueba\n",
        "    y_pred = xgb_best_model.predict(X_test)\n",
        "\n",
        "    # Calcular el error cuadrático medio (MSE) del modelo en el grupo de prueba\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    # Calcular el coeficiente de determinación (R2) del modelo en el grupo de prueba\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Calcular el error porcentual absoluto medio (MAPE) del modelo en el grupo de prueba\n",
        "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "\n",
        "    return xgb_best_model,y_pred, best_params, mse, r2, mape"
      ],
      "metadata": {
        "id": "GogvNWQB9kVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_forest_regression(X_train, X_test, y_train, y_test, param_grid):\n",
        "    # Paso 1: Definir el modelo de Random Forest para regresión\n",
        "    rf_model = RandomForestRegressor()\n",
        "\n",
        "    # Paso 2: Realizar la búsqueda de hiperparámetros utilizando GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Paso 3: Obtener el mejor modelo y los mejores parámetros\n",
        "    xgb_best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Paso 4: Realizar predicciones y calcular las métricas de evaluación\n",
        "    y_pred = xgb_best_model.predict(X_test)\n",
        "\n",
        "    return xgb_best_model, y_pred, best_params"
      ],
      "metadata": {
        "id": "OULIb4S-QR_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_forest_model_evaluation(X_train, X_test, y_train, y_test,param_grid):\n",
        "    \"\"\"\n",
        "    Genera la predicción y evaluación de un modelo RandomForestRegressor utilizando Grid Search para encontrar los mejores hiperparámetros.\n",
        "\n",
        "    Parámetros:\n",
        "    - X_train: matriz de características para el grupo de entrenamiento.\n",
        "    - X_test: matriz de características para el grupo de prueba.\n",
        "    - y_train: vector de etiquetas para el grupo de entrenamiento.\n",
        "    - y_test: vector de etiquetas para el grupo de prueba.\n",
        "\n",
        "    Retorna:\n",
        "    - y_pred: predicciones del modelo en el grupo de prueba.\n",
        "    - best_params: mejores hiperparámetros encontrados mediante Grid Search.\n",
        "    - accuracy: exactitud (accuracy) del modelo en el grupo de prueba.\n",
        "    \"\"\"\n",
        "\n",
        "    # Crear el modelo RandomForestRegressor\n",
        "    rf_model = RandomForestRegressor()\n",
        "\n",
        "    # Realizar Grid Search con validación cruzada\n",
        "    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Obtener los mejores hiperparámetros\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Crear el modelo RandomForestRegressor con los mejores hiperparámetros\n",
        "    rf_best_model = RandomForestRegressor(**best_params)\n",
        "\n",
        "    # Ajustar el modelo con los mejores hiperparámetros a los datos de entrenamiento\n",
        "    rf_best_model.fit(X_train, y_train)\n",
        "\n",
        "    # Realizar la predicción en el grupo de prueba\n",
        "    y_pred = rf_best_model.predict(X_test)\n",
        "\n",
        "    # Calcular la exactitud del modelo en el grupo de prueba\n",
        "    #accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return y_pred, best_params\n",
        "print('Random Forest Loaded')"
      ],
      "metadata": {
        "id": "toHP5vZk-wN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bd81b8e-affc-402f-b7ff-5d0a37dd9895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def obtener_resultados_modelo(modelo, X, y):\n",
        "    # Obtener las etiquetas predichas por el modelo\n",
        "    y_pred = modelo.predict(X)\n",
        "\n",
        "    # Convertir 'y_pred' de ndarray a DataFrame\n",
        "    y_pred_df = pd.DataFrame(y_pred, columns=['y_pred'])\n",
        "\n",
        "    # Reiniciar los índices para asegurar la alineación correcta\n",
        "    y.reset_index(drop=True, inplace=True)\n",
        "    y_pred_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Concatenar 'y' y 'y_pred' a lo largo de las columnas\n",
        "    result_df = pd.concat([y, y_pred_df], axis=1)\n",
        "    r2 = r2_score(result_df['Crecimiento'], result_df['y_pred'])\n",
        "    mse = mean_squared_error(result_df['Crecimiento'], result_df['y_pred'])\n",
        "    mape = mape = np.mean(np.abs((result_df['Crecimiento']- result_df['y_pred']) / result_df['Crecimiento'])) * 100\n",
        "\n",
        "    # Calcular la diferencia entre 'y' y 'y_pred'\n",
        "    result_df['diferencia'] = result_df['Crecimiento'] - result_df['y_pred']\n",
        "    result_df['diferencia2'] = (result_df['Crecimiento'] - result_df['y_pred'])/result_df['Crecimiento']\n",
        "\n",
        "    return result_df,r2,mse,mape"
      ],
      "metadata": {
        "id": "ZHihIKjHu0gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Función que elimina las tildes y diacríticos de un texto.\n",
        "def quitar_tildes(texto):\n",
        "    texto_sin_tildes = unidecode(texto)\n",
        "    return texto_sin_tildes\n",
        "print('quitar_tildes Loaded')"
      ],
      "metadata": {
        "id": "jgZaT4XxU0vc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c46dda-e3dc-4418-b941-2e46042a35a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quitar_tildes Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Función completa datos faltantes de serie de timepo\n",
        "def completar_datos_serie_tiempo(df,inicio,fin):\n",
        "\n",
        "    # Convert 'Year' to datetime type with date set to December 31st\n",
        "    df['Year'] = pd.to_datetime(df['Year'].astype(str) + '-12-31')\n",
        "\n",
        "    # Set 'Year' and 'Category' as the index\n",
        "    df.set_index(['Year', 'Category'], inplace=True)\n",
        "\n",
        "    # Create a new DataFrame with all years and categories from the first year to the last year in the available data\n",
        "    start_year = inicio\n",
        "    end_year = fin\n",
        "    all_years = pd.DataFrame(index=pd.MultiIndex.from_product([[str(y) + '-12-31' for y in range(start_year, end_year+1)], df.index.get_level_values('Category').unique()], names=['Year', 'Category']))\n",
        "\n",
        "\n",
        "    all_years['join']=all_years.index.get_level_values('Year').astype(str) + '-' + all_years.index.get_level_values('Category').astype(str)\n",
        "    df['join']=df.index.get_level_values('Year').astype(str) + '-' + df.index.get_level_values('Category').astype(str)\n",
        "\n",
        "\n",
        "    # Join the existing DataFrame with all_years\n",
        "    all_years.reset_index(inplace=True)\n",
        "    all_years['Year'] = pd.to_datetime(all_years['Year'])\n",
        "    df_complete = pd.merge(all_years, df,on='join',how='left')\n",
        "    #df_complete_s=df_complete\n",
        "    df_complete.set_index(['Year', 'Category'], inplace=True)\n",
        "    df_complete = df_complete.drop('join', axis=1)\n",
        "\n",
        "\n",
        "    # Obtener la lista de columnas que necesitan ser completadas\n",
        "    columns_to_interpolate = df_complete.columns.tolist()\n",
        "\n",
        "\n",
        "    for column in columns_to_interpolate:\n",
        "        df_complete[column] = df_complete.groupby('Category',group_keys=False)[column].apply(lambda x: x.interpolate(limit_direction='both'))\n",
        "\n",
        "    df_complete.reset_index(inplace=True)\n",
        "    df_complete=df_complete.fillna(0)\n",
        "    df_complete['Year'] = df_complete['Year'].dt.year\n",
        "\n",
        "\n",
        "    return df_complete\n",
        "print('completar_datos_serie_tiempo Loaded')"
      ],
      "metadata": {
        "id": "qf9sSh0E1Lnp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac8fc56-6712-4063-f5d6-26d87a2518f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "completar_datos_serie_tiempo Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#elimina las filas con NAN\n",
        "def eliminar_filas_nan(dataframe, columna):\n",
        "    dataframe_sin_nan = dataframe.dropna(subset=[columna], inplace=False)\n",
        "    return dataframe_sin_nan\n",
        "print('eliminar_filas_nan Loaded')"
      ],
      "metadata": {
        "id": "_xF-IntM4AI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14448dab-b9b7-458b-af81-31677b42650b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eliminar_filas_nan Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#si encuentra numeros menores los convierte en Nan\n",
        "def cambiar_numeros_menores(dataframe, valor, columna_excluida):\n",
        "    for columna in dataframe.columns:\n",
        "        if columna != columna_excluida:\n",
        "            dataframe[columna] = np.where(dataframe[columna] < valor, np.nan, dataframe[columna])\n",
        "    return dataframe\n",
        "print('cambiar_numeros_menores Loaded')"
      ],
      "metadata": {
        "id": "I2ddE96T6M1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4b78f6b-56a9-49fc-91ef-84b78569827e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cambiar_numeros_menores Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Funcion que Normaliza variables numericas\n",
        "\n",
        "def normalize_variables(dataset):\n",
        "    scaler = MinMaxScaler()\n",
        "    normalized_dataset = scaler.fit_transform(dataset)\n",
        "    normalized_dataset = pd.DataFrame(normalized_dataset, columns=dataset.columns)\n",
        "    return normalized_dataset\n",
        "print('normalize_variables Loaded')"
      ],
      "metadata": {
        "id": "2cw4m53od6iy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e281a9-e096-4332-8f88-ec474e8a017c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normalize_variables Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_variables_limpieza(dataset):\n",
        "    # Eliminar valores infinitos o muy grandes\n",
        "    dataset = dataset.replace([np.inf, -np.inf], np.nan)\n",
        "    dataset = dataset.fillna(0)  # Reemplazar valores NaN por cero o cualquier otro valor apropiado\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    normalized_dataset = scaler.fit_transform(dataset)\n",
        "    normalized_dataset = pd.DataFrame(normalized_dataset, columns=dataset.columns)\n",
        "    return normalized_dataset"
      ],
      "metadata": {
        "id": "lzNla__N3tAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calcula el crecimiento respecto a un año especifico\n",
        "def calcular_crecimiento_ingresos(data, years_offset,Columna,ingresos,Yearp):\n",
        "    data_sorted = data.sort_values(by=[Yearp])\n",
        "    data_pivot = data_sorted.pivot(index=Columna, columns=Yearp, values=ingresos)\n",
        "\n",
        "    # Obtener todas las columnas de años en el DataFrame\n",
        "    years = data_pivot.columns.tolist()\n",
        "\n",
        "    crecimiento_data = []\n",
        "\n",
        "    if years_offset < 0:\n",
        "        years_offset = abs(years_offset)\n",
        "        for i in range(years_offset, len(years)):\n",
        "            current_year = years[i]\n",
        "            past_year = years[i - years_offset]\n",
        "\n",
        "            crecimiento = (data_pivot[current_year] - data_pivot[past_year]) / data_pivot[past_year]\n",
        "\n",
        "            crecimiento_data.append(pd.DataFrame({Columna: data_pivot.index, Yearp: current_year, 'Crecimiento': crecimiento}))\n",
        "    else:\n",
        "        for i in range(len(years) - years_offset):\n",
        "            current_year = years[i]\n",
        "            future_year = years[i + years_offset]\n",
        "\n",
        "            crecimiento = (data_pivot[future_year] - data_pivot[current_year]) / data_pivot[current_year]\n",
        "\n",
        "            crecimiento_data.append(pd.DataFrame({Columna: data_pivot.index, Yearp: current_year, 'Crecimiento': crecimiento}))\n",
        "\n",
        "    result = pd.concat(crecimiento_data, ignore_index=True)\n",
        "\n",
        "    return result\n",
        "print('calcular_crecimiento_ingresos Loaded')"
      ],
      "metadata": {
        "id": "J126QWVdK-D5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b897e10b-16dd-4a96-f807-b89a251e5886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calcular_crecimiento_ingresos Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Genera grupos de variables que comparten alta correlacion\n",
        "def obtener_variables_correlacionadas(data, umbral):\n",
        "    # Calcular la matriz de correlación\n",
        "    matriz_correlacion = data.corr().abs()\n",
        "\n",
        "    # Crear un diccionario para mapear las variables con sus grupos\n",
        "    grupos = {}\n",
        "    num_grupo = 1\n",
        "\n",
        "    # Recorrer las variables y asignarles el número de grupo\n",
        "    for variable in matriz_correlacion.columns:\n",
        "        if variable not in grupos:\n",
        "            variables_correlacionadas = matriz_correlacion.index[matriz_correlacion[variable] >= umbral]\n",
        "            for var in variables_correlacionadas:\n",
        "                grupos[var] = num_grupo\n",
        "            num_grupo += 1\n",
        "\n",
        "    # Crear un DataFrame con las variables agrupadas\n",
        "    df_variables_correlacionadas = pd.DataFrame({'Variable': list(grupos.keys()), 'Grupo': list(grupos.values())})\n",
        "\n",
        "    return df_variables_correlacionadas\n",
        "print('obtener_variables_correlacionadas Loaded')"
      ],
      "metadata": {
        "id": "2tf02cgYltoh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51ff1e0d-8688-4590-a22d-9fa913f390ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "obtener_variables_correlacionadas Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generar_componentes_principales(df_datos, df_grupos):\n",
        "    grupos = df_grupos['Grupo'].unique()  # Obtener la lista de grupos únicos\n",
        "    componentes_p = pd.DataFrame()\n",
        "    for grupo in grupos:\n",
        "        # Filtrar las variables correspondientes al grupo actual\n",
        "        variables_grupo = df_grupos[df_grupos['Grupo'] == grupo]['Variable'].tolist()\n",
        "\n",
        "        # Filtrar las columnas del DataFrame de datos según las variables del grupo actual\n",
        "        datos_grupo = df_datos[variables_grupo]\n",
        "\n",
        "        # Aplicar PCA al DataFrame del grupo actual\n",
        "        pca = PCA(n_components=1)\n",
        "        componente_principal = pca.fit_transform(datos_grupo)\n",
        "\n",
        "        # Asignar nombre al componente principal\n",
        "        nombre_componente = \"Componente_Principal_\" + str(grupo)\n",
        "        df_temp = pd.DataFrame(componente_principal, columns=[nombre_componente])\n",
        "        componentes_p = pd.concat([componentes_p, df_temp], axis=1)\n",
        "        # Agregar el componente principal al DataFrame de datos\n",
        "\n",
        "    return componentes_p\n",
        "print('generar_componentes_principales Loaded')"
      ],
      "metadata": {
        "id": "tao3eNYRP4Dw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b471c8-acc3-4e15-9e59-97e9b03d315d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generar_componentes_principales Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RFE: Fucnion que obtiene las variables utilizanfo el modelo de Recursive Feature Elimination.\n",
        "\n",
        "def rfe_with_elasticnet(X, y, n_features_to_select):\n",
        "    # Crear el estimador ElasticNet\n",
        "    estimator = ElasticNet()\n",
        "\n",
        "    # Crear el objeto RFE\n",
        "    rfe = RFE(estimator=estimator, n_features_to_select=n_features_to_select)\n",
        "\n",
        "    # Realizar la selección de características\n",
        "    rfe.fit(X, y)\n",
        "\n",
        "    # Obtener los nombres de las características\n",
        "    feature_names = X.columns\n",
        "\n",
        "    # Obtener los coeficientes de importancia de las características\n",
        "    feature_importances = rfe.estimator_.coef_\n",
        "\n",
        "    # Crear un DataFrame con las características y sus coeficientes de importancia\n",
        "    result_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "\n",
        "    return result_df\n",
        "print('rfe_with_elasticnet Loaded')"
      ],
      "metadata": {
        "id": "5ySQGq29FnU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funcion para calcular RFE En classificación\n",
        "\n",
        "def select_features_with_rfe_Clasisification(X, y, num_features):\n",
        "    # Instanciar el modelo de clasificación (por ejemplo, Regresión Logística)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "    # Crear el objeto RFE y especificar el modelo y el número de características deseado\n",
        "    rfe = RFE(model, n_features_to_select=num_features)\n",
        "\n",
        "    # Ajustar RFE a los datos\n",
        "    rfe.fit(X, y)\n",
        "\n",
        "    # Obtener las características seleccionadas con sus coeficientes de importancia\n",
        "    selected_features = X.columns[rfe.support_]\n",
        "    coef_importance = rfe.estimator_.coef_[0]\n",
        "\n",
        "    # Crear un DataFrame con las características seleccionadas y sus coeficientes de importancia\n",
        "    result_df = pd.DataFrame({'Variable': selected_features, 'Importance': coef_importance})\n",
        "\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "epsNE8xi4xQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calcular_crecimiento_promedio_por_categoria_Predial(df, Year_inicio_periodo1, Year_fin_periodo1, Year_inicio_periodo2, Year_fin_periodo2):\n",
        "    # Filtrar el DataFrame por los años de cada periodo\n",
        "    periodo1 = df[(df['Año'] >= Year_inicio_periodo1) & (df['Año'] <= Year_fin_periodo1)]\n",
        "    periodo2 = df[(df['Año'] >= Year_inicio_periodo2) & (df['Año'] <= Year_fin_periodo2)]\n",
        "\n",
        "    # Calcular el promedio de valores para cada categoría en cada periodo\n",
        "    promedio_periodo1 = periodo1.groupby('CODIGO_DANE')['Predial'].mean()\n",
        "    promedio_periodo2 = periodo2.groupby('CODIGO_DANE')['Predial'].mean()\n",
        "\n",
        "    # Calcular el crecimiento porcentual promedio para cada categoría\n",
        "    crecimiento_promedio = ((promedio_periodo2 - promedio_periodo1) / promedio_periodo1) * 100\n",
        "\n",
        "    return crecimiento_promedio\n",
        "print('calcular_crecimiento_promedio_por_categoria_Predial Loaded')"
      ],
      "metadata": {
        "id": "_uLt92HqFnIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categorizar_por_boxplot(df,columna_valores):\n",
        "    # Calcular el primer, segundo y tercer cuartil\n",
        "    primer_cuartil = columna_valores.quantile(0.25)\n",
        "    segundo_cuartil = columna_valores.quantile(0.5)\n",
        "    tercer_cuartil = columna_valores.quantile(0.75)\n",
        "\n",
        "    # Calcular los límites del bigote inferior y superior\n",
        "    iqr = tercer_cuartil - primer_cuartil\n",
        "    limite_inferior = primer_cuartil - 1.5 * iqr\n",
        "    limite_superior = tercer_cuartil + 1.5 * iqr\n",
        "\n",
        "    # Categorizar los valores según los análisis de boxplot\n",
        "    categorias = []\n",
        "    for valor in columna_valores:\n",
        "        if valor < limite_inferior:\n",
        "            categorias.append('Muy baja')\n",
        "        elif valor < primer_cuartil:\n",
        "            categorias.append('Baja')\n",
        "        elif valor < tercer_cuartil:\n",
        "            categorias.append('Media')\n",
        "        elif valor <= limite_superior:\n",
        "            categorias.append('Alta')\n",
        "        else:\n",
        "            categorias.append('Muy alta')\n",
        "    df['recomendacion'] = categorias\n",
        "    return categorias\n",
        "print('categorizar_por_boxplot Loaded')"
      ],
      "metadata": {
        "id": "If8z7nT_LEcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calcular_crecimiento_promedio_por_categoria(df, columnas_procesar, Year_inicio_periodo1, Year_fin_periodo1, Year_inicio_periodo2, Year_fin_periodo2):\n",
        "    # Filtrar el DataFrame por los años de cada periodo\n",
        "    periodo1 = df[(df['Year'] >= Year_inicio_periodo1) & (df['Year'] <= Year_fin_periodo1)]\n",
        "    periodo2 = df[(df['Year'] >= Year_inicio_periodo2) & (df['Year'] <= Year_fin_periodo2)]\n",
        "\n",
        "    # Crear un DataFrame vacío para almacenar los resultados\n",
        "    resultados = pd.DataFrame()\n",
        "\n",
        "    # Iterar sobre las columnas y calcular el crecimiento por categoría\n",
        "    for columna in columnas_procesar:\n",
        "        # Calcular el promedio de valores para cada categoría en cada periodo\n",
        "        promedio_periodo1 = periodo1.groupby('Municipio')[columna].mean()\n",
        "        promedio_periodo2 = periodo2.groupby('Municipio')[columna].mean()\n",
        "\n",
        "        # Calcular el crecimiento porcentual promedio para cada categoría\n",
        "        crecimiento_promedio = ((promedio_periodo2 - promedio_periodo1) / promedio_periodo1) * 100\n",
        "\n",
        "        # Agregar el resultado al DataFrame resultados\n",
        "        resultados = pd.concat([resultados, crecimiento_promedio], axis=1)\n",
        "\n",
        "    # Renombrar las columnas con los nombres originales\n",
        "    resultados.columns = columnas_procesar\n",
        "\n",
        "    return resultados\n",
        "print('calcular_crecimiento_promedio_por_categoria Loaded')"
      ],
      "metadata": {
        "id": "gmgfBgDmcvSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def limitar_atipicos(df, columnas_procesar):\n",
        "    # Inicializar un DataFrame para almacenar los resultados\n",
        "    resultados = df.copy()\n",
        "\n",
        "    # Iterar sobre las columnas a procesar\n",
        "    for columna in columnas_procesar:\n",
        "        # Calcular el rango intercuartílico (IQR)\n",
        "        q1 = df[columna].quantile(0.25)\n",
        "        q3 = df[columna].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "\n",
        "        # Calcular los límites del bigote superior e inferior\n",
        "        upper_limit = q3 + 1.5 * iqr\n",
        "        lower_limit = q1 - 1.5 * iqr\n",
        "\n",
        "        # Reemplazar los valores atípicos con los límites del bigote\n",
        "        resultados[columna] = resultados[columna].apply(lambda x: upper_limit if x > upper_limit else lower_limit if x < lower_limit else x)\n",
        "\n",
        "    return resultados\n",
        "print('limitar_atipicos Loaded')"
      ],
      "metadata": {
        "id": "IS7-CU7AcwQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_dummies(df, columns_to_dummy):\n",
        "    for col in columns_to_dummy:\n",
        "        dummies = pd.get_dummies(df[col], prefix=col)\n",
        "        df = pd.concat([df, dummies], axis=1)\n",
        "        df.drop(col, axis=1, inplace=True)\n",
        "    return df\n",
        "print('crear_dummies Loaded')"
      ],
      "metadata": {
        "id": "EGoW3QaFeyDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Funcion para comvertir un data frame con años en columasn en filas y generar comparacion de fechas\n",
        "def year_columnas(Industria, palabra,ID,Year_inicio_periodo1, Year_fin_periodo1, Year_inicio_periodo2, Year_fin_periodo2):\n",
        "\n",
        "    new_columns = [col.replace(palabra, '') for col in Industria.columns] # quita un valor de una columna\n",
        "    Industria.rename(columns=dict(zip(Industria.columns, new_columns)), inplace=True) # quita un valor de una columna\n",
        "\n",
        "    Industria = Industria.melt(id_vars=[ID], var_name='Year', value_name='Valor') #Transpone los años en filas\n",
        "\n",
        "    Industria.rename(columns={ID: 'Category'}, inplace=True) #Cambio de nombre de columna\n",
        "    Industria=completar_datos_serie_tiempo(Industria,2008,2020) #Completa faltantes con interpolación\n",
        "\n",
        "    Industria.rename(columns={'Category': 'Municipio'}, inplace=True) #Cambio de nombre de columna\n",
        "    Industria.rename(columns={'Valor': 'Industria'}, inplace=True) #Cambio de nombre de columna\n",
        "    lista_columnas = Industria[Industria.columns.difference(['Municipio','Year'])].columns.tolist()\n",
        "    Industria=calcular_crecimiento_promedio_por_categoria(Industria,lista_columnas,Year_inicio_periodo1, Year_fin_periodo1, Year_inicio_periodo2, Year_fin_periodo2)\n",
        "    Industria=limitar_atipicos(Industria, lista_columnas)\n",
        "    Industria = Industria.fillna(0)\n",
        "    Industria=Industria.reset_index()\n",
        "    Industria.rename(columns={'index': 'Municipio'}, inplace=True) #Cambio de nombre de columna\n",
        "    return Industria"
      ],
      "metadata": {
        "id": "lFMDSN1S66bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(X_train, X_test, y_train, y_test, Modelo, param_grid=None, random_state=13):\n",
        "    if Modelo not in ['xgboost', 'random_forest']:\n",
        "        raise ValueError('Modelo debe ser \"xgboost\" o \"random_forest\".')\n",
        "\n",
        "    if Modelo == 'xgboost':\n",
        "        model = xgb.XGBClassifier(tree_method='gpu_hist', random_state=random_state)\n",
        "        model = xgb.XGBClassifier(random_state=random_state)\n",
        "    else:\n",
        "        model = RandomForestClassifier(random_state=random_state)\n",
        "\n",
        "    if param_grid is not None:\n",
        "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='roc_auc_ovr', cv=5, n_jobs=-1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        best_params = grid_search.best_params_\n",
        "        model.set_params(**best_params)\n",
        "\n",
        "    # Validación cruzada (k-fold cross-validation)\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc_ovr', n_jobs=-1)\n",
        "    print(\"Mean CV ROC AUC:\", cv_scores.mean())\n",
        "\n",
        "    # Curva de aprendizaje\n",
        "    #train_sizes, train_scores, test_scores = learning_curve(model, X_train, y_train, cv=5, scoring='roc_auc_ovr', n_jobs=-1)\n",
        "    train_sizes, train_scores, test_scores = learning_curve(model, X_train, y_train, cv=5, scoring='roc_auc_ovr', n_jobs=-1)\n",
        "    plt.figure()\n",
        "    plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', label='Training Score')\n",
        "    plt.plot(train_sizes, test_scores.mean(axis=1), 'o-', label='Test Score')\n",
        "    plt.xlabel('Training examples')\n",
        "    plt.ylabel('ROC AUC')\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    with tf.device('/GPU:0'):\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr')\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    return model, accuracy, roc_auc, conf_matrix, best_params"
      ],
      "metadata": {
        "id": "0CeIsOx81ANE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_eval, y_eval):\n",
        "    # Obtener las predicciones del modelo\n",
        "    y_pred = model.predict(X_eval)\n",
        "\n",
        "    # Calcular el accuracy\n",
        "    accuracy = accuracy_score(y_eval, y_pred)\n",
        "\n",
        "    # Calcular el valor de la curva ROC\n",
        "    #roc_auc = roc_auc_score(y_eval, y_pred,multi_class='ovr')\n",
        "    roc_auc = roc_auc_score(y_eval, model.predict_proba(X_eval), multi_class='ovr')\n",
        "    # Calcular la matriz de confusión\n",
        "    conf_matrix = confusion_matrix(y_eval, y_pred)\n",
        "\n",
        "    # Crear el DataFrame con los resultados\n",
        "    result_df = pd.DataFrame({\n",
        "        'Accuracy': [accuracy],\n",
        "        'ROC_AUC': [roc_auc]\n",
        "    })\n",
        "\n",
        "    return result_df,conf_matrix"
      ],
      "metadata": {
        "id": "TJNFW6h31XZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def obtener_clasificacion_y_scores(modelo, X, lista_id):\n",
        "    \"\"\"\n",
        "    Obtiene la clasificación de y y el score de cada categoría para un conjunto de datos X.\n",
        "\n",
        "    Parámetros:\n",
        "        modelo: Modelo entrenado capaz de realizar predicciones multiclase.\n",
        "        X: Conjunto de datos de entrada para realizar las predicciones.\n",
        "        lista_id (list): Lista de identificadores (ID) correspondientes a los datos en X.\n",
        "\n",
        "    Retorna:\n",
        "        pandas DataFrame: Un DataFrame con las columnas ID, y, y las scores para cada categoría.\n",
        "    \"\"\"\n",
        "    # Realizar predicciones para el conjunto de datos X\n",
        "    y_pred_prob = modelo.predict_proba(X)\n",
        "\n",
        "    # Obtener las categorías del modelo\n",
        "    categorias = modelo.classes_\n",
        "\n",
        "    # Crear una lista con el resultado para cada fila\n",
        "    resultados = []\n",
        "    for i in range(len(X)):\n",
        "        fila = [lista_id[i]] + [categorias[np.argmax(y_pred_prob[i])]] + y_pred_prob[i].tolist()\n",
        "        resultados.append(fila)\n",
        "\n",
        "    # Crear el DataFrame con los resultados\n",
        "    columnas = ['ID', 'y'] + [f'Score_{categoria}' for categoria in categorias]\n",
        "    df_resultados = pd.DataFrame(resultados, columns=columnas)\n",
        "\n",
        "    return df_resultados"
      ],
      "metadata": {
        "id": "o3FpTWLy8VwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Funcion que elimina atipicos superiores y los reeemplazo por el valor mayor\n",
        "def identificar_y_reemplazar_atipicos_superiores(dataframe):\n",
        "    # Copiar el DataFrame original para no modificarlo directamente\n",
        "    df = dataframe.copy()\n",
        "\n",
        "    # Iterar sobre cada columna del DataFrame\n",
        "    for columna in df.columns:\n",
        "        # Calcular los límites del boxplot para la columna actual\n",
        "        q1 = df[columna].quantile(0.25)\n",
        "        q3 = df[columna].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        limite_superior = q3 + 1.5 * iqr\n",
        "\n",
        "        # Identificar los valores atípicos superiores en la columna actual\n",
        "        valores_atipicos_superiores = df[columna] > limite_superior\n",
        "\n",
        "        # Reemplazar los valores atípicos superiores por el límite superior del boxplot\n",
        "        df.loc[valores_atipicos_superiores, columna] = limite_superior\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "Y1kIrihH0t1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Funcion que ajsuta una funcion especifica para completar atipicos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "# Definir algunas funciones para probar\n",
        "def funcion_lineal(x, a, b):\n",
        "    return a * x + b\n",
        "\n",
        "def funcion_exponencial(x, a, b):\n",
        "    return a * np.exp(b * x)\n",
        "\n",
        "def funcion_polinomica(x, a, b, c):\n",
        "    return a * x**2 + b * x + c\n",
        "\n",
        "def funcion_logaritmica(x, a, b):\n",
        "    return a * np.log(x) + b\n",
        "\n",
        "def funcion_coseno(x, a, b):\n",
        "    return a * np.cos(b * x)\n",
        "\n",
        "def funcion_seno(x, a, b):\n",
        "    return a * np.sin(b * x)\n",
        "\n",
        "def funcion_cuadratica(x, a, b, c):\n",
        "    return a * x**2 + b * x + c\n",
        "\n",
        "def funcion_cubica(x, a, b, c, d):\n",
        "    return a * x**3 + b * x**2 + c * x + d\n",
        "\n",
        "def funcion_raiz_cuadrada(x, a, b):\n",
        "    return a * np.sqrt(x) + b\n",
        "\n",
        "def funcion_logistica(x, a, b, c):\n",
        "    return c / (1 + np.exp(-(x - b) / a))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "def ajustar_funcion(df, columna_fecha, columna_valor, funciones, tolerancia_porcentaje):\n",
        "    mejores_resultados = None\n",
        "    mejor_mse = np.inf\n",
        "\n",
        "\n",
        "    # Convertir las fechas a números enteros que representen el año\n",
        "    df['Anio'] = df[columna_fecha].dt.year\n",
        "\n",
        "    # Obtener los valores de año y valor como arrays\n",
        "    anios = df['Anio'].values\n",
        "    valores = df[columna_valor].values\n",
        "\n",
        "    for funcion in funciones:\n",
        "        # Ajustar la función a los datos de la serie de tiempo\n",
        "        parametros_optimizados, _ = curve_fit(funcion, anios, valores,maxfev=20000)\n",
        "\n",
        "        # Calcular los valores ajustados utilizando la función y los parámetros optimizados\n",
        "        valores_ajustados = funcion(anios, *parametros_optimizados)\n",
        "\n",
        "        # Calcular la diferencia entre los valores reales y los ajustados en porcentaje\n",
        "        diferencia_porcentaje = 100 * np.abs((valores - valores_ajustados) / valores)\n",
        "\n",
        "        # Reemplazar los valores que están fuera de la tolerancia con los valores ajustados\n",
        "        valores_corregidos = np.where(diferencia_porcentaje > tolerancia_porcentaje, valores_ajustados, valores)\n",
        "\n",
        "        # Calcular el error cuadrático medio entre los valores reales y los ajustados\n",
        "        mse = np.mean((valores - valores_ajustados)**2)\n",
        "\n",
        "        # Verificar si esta función es la de mejor ajuste hasta ahora\n",
        "        if mse < mejor_mse:\n",
        "            mejores_resultados = valores_corregidos\n",
        "            mejor_mse = mse\n",
        "\n",
        "    # Agregar una columna al DataFrame original con los valores ajustados de la mejor función\n",
        "    df['Valores_Ajustados'] = mejores_resultados\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def ajustar_funcion2(df, columna_fecha, columna_valor,columna_categoria, funciones, tolerancia_porcentaje ):\n",
        "    df_resultado = df.copy()\n",
        "\n",
        "    # Convertir las fechas a números enteros que representen el año\n",
        "    df_resultado['Anio'] = df_resultado[columna_fecha].dt.year\n",
        "\n",
        "    # Iterar por cada categoría única en la columna de categorías\n",
        "    for categoria in df_resultado[columna_categoria].unique():\n",
        "        df_categoria = df_resultado[df_resultado[columna_categoria] == categoria]\n",
        "\n",
        "        # Obtener los valores de año y valor como arrays para la categoría actual\n",
        "        anios_categoria = df_categoria['Anio'].values\n",
        "        valores_categoria = df_categoria[columna_valor].values\n",
        "\n",
        "        mejores_resultados = None\n",
        "        mejor_mse = np.inf\n",
        "        try:\n",
        "          for funcion in funciones:\n",
        "              #print(categoria)\n",
        "              # Ajustar la función a los datos de la serie de tiempo de la categoría actual\n",
        "              parametros_optimizados, _ = curve_fit(funcion, anios_categoria, valores_categoria, maxfev=100000)\n",
        "\n",
        "              # Calcular los valores ajustados utilizando la función y los parámetros optimizados\n",
        "              valores_ajustados = funcion(anios_categoria, *parametros_optimizados)\n",
        "\n",
        "              # Calcular la diferencia entre los valores reales y los ajustados en porcentaje\n",
        "              diferencia_porcentaje = 100 * np.abs((valores_categoria - valores_ajustados) / valores_categoria)\n",
        "\n",
        "              # Reemplazar los valores que están fuera de la tolerancia con los valores ajustados\n",
        "              valores_corregidos = np.where(diferencia_porcentaje > tolerancia_porcentaje, valores_ajustados, valores_categoria)\n",
        "\n",
        "              # Calcular el error cuadrático medio entre los valores reales y los ajustados\n",
        "              mse = np.mean((valores_categoria - valores_ajustados)**2)\n",
        "\n",
        "              # Verificar si esta función es la de mejor ajuste hasta ahora\n",
        "              if mse < mejor_mse:\n",
        "                  mejores_resultados = valores_corregidos\n",
        "                  mejor_mse = mse\n",
        "        except Exception as e:\n",
        "            # Capturar cualquier excepción que ocurra y continuar con el siguiente grupo\n",
        "            print(f\"Se produjo un error en el grupo '{categoria}': {str(e)}\")\n",
        "            continue\n",
        "        # Agregar una columna al DataFrame de la categoría actual con los valores ajustados de la mejor función\n",
        "        df_resultado.loc[df_resultado[columna_categoria] == categoria, 'Valores_Ajustados'] = mejores_resultados\n",
        "\n",
        "    return df_resultado"
      ],
      "metadata": {
        "id": "3sKaIMa4sxPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Funcion que genera una rpediccion de arima\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "def generar_prediccion_arima(data, categoria, columna_fecha, columna_valor):\n",
        "    # Filtrar los datos para la categoría y el rango de fechas deseados\n",
        "    df_categoria = data[data['Categoria'] == categoria].copy()\n",
        "    df_categoria[columna_fecha] = pd.to_datetime(df_categoria[columna_fecha])\n",
        "    df_categoria.set_index(columna_fecha, inplace=True)\n",
        "\n",
        "    # Dividir los datos en entrenamiento (2008-2017) y prueba (2018-2030)\n",
        "    train_data = df_categoria['2008':'2017'][columna_valor]\n",
        "    test_data = df_categoria['2018':'2030'][columna_valor]\n",
        "\n",
        "    # Ajustar el modelo ARIMA para la serie de tiempo de entrenamiento\n",
        "    modelo_arima = ARIMA(train_data, order=(1, 1, 0))\n",
        "    resultado = modelo_arima.fit()\n",
        "\n",
        "    # Generar el pronóstico para los próximos 13 años (2018-2030)\n",
        "    pronostico_anual = resultado.forecast(steps=13)\n",
        "\n",
        "    # Crear un rango de años para los próximos 13 años (2018-2030)\n",
        "    fechas_pronostico = pd.date_range(df_categoria.index[-1], periods=13, freq='Y')\n",
        "\n",
        "    # Crear un DataFrame con el pronóstico y los datos reales\n",
        "    df_pronostico = pd.DataFrame({columna_fecha: fechas_pronostico, 'Pronostico': pronostico_anual})\n",
        "    df_pronostico.set_index(columna_fecha, inplace=True)\n",
        "\n",
        "    # Completar los datos reales del 2008 al 2017 en el DataFrame de pronóstico\n",
        "    df_pronostico['Real'] = df_categoria['2008':'2017'][columna_valor]\n",
        "\n",
        "    return df_pronostico"
      ],
      "metadata": {
        "id": "TaiypXYRs7-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Quita atipicos por Boxplot y lso completa interpolando\n",
        "\n",
        "def normalizar_serie_tiempo_por_categoria(df, fecha_col, categoria_col, valor_col):\n",
        "    \"\"\"\n",
        "    Normaliza una serie de tiempo dentro de un DataFrame para cada categoría.\n",
        "\n",
        "    Parámetros:\n",
        "        - df: DataFrame con los datos.\n",
        "        - fecha_col: Nombre de la columna que contiene las fechas.\n",
        "        - categoria_col: Nombre de la columna que contiene las categorías.\n",
        "        - valor_col: Nombre de la columna que contiene los valores de la serie de tiempo.\n",
        "\n",
        "    Retorna:\n",
        "        - DataFrame con la serie de tiempo normalizada por categoría.\n",
        "    \"\"\"\n",
        "\n",
        "    def eliminar_outliers_interpolacion(serie_tiempo):\n",
        "        q1 = serie_tiempo.quantile(0.25)\n",
        "        q3 = serie_tiempo.quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        limite_inferior = q1 - 1.5 * iqr\n",
        "        limite_superior = q3 + 1.5 * iqr\n",
        "        serie_tiempo_sin_outliers = serie_tiempo.mask((serie_tiempo < limite_inferior) | (serie_tiempo > limite_superior), other=None)\n",
        "        serie_tiempo_completada = serie_tiempo_sin_outliers.interpolate(method='linear', limit_direction='both')\n",
        "        return serie_tiempo_completada\n",
        "\n",
        "    df_resultado = pd.DataFrame()\n",
        "    #df=df.replace(0, np.nan, inplace=True)\n",
        "    # Iterar por cada categoría única en el DataFrame\n",
        "    for categoria in df[categoria_col].unique():\n",
        "        # Filtrar los datos para la categoría actual\n",
        "        df_categoria = df[df[categoria_col] == categoria].copy()\n",
        "\n",
        "        # Eliminar valores atípicos y completar datos faltantes para la serie de tiempo de la categoría actual\n",
        "        df_categoria[valor_col] = eliminar_outliers_interpolacion(df_categoria[valor_col])\n",
        "\n",
        "        # Agregar los datos de la categoría actual al DataFrame resultado\n",
        "        df_resultado = pd.concat([df_resultado, df_categoria])\n",
        "\n",
        "    return df_resultado"
      ],
      "metadata": {
        "id": "gEMG4x_qtADB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Arima 10 años\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "def ajustar_serie_tiempo_por_categoria(data, columna_categoria, columna_fecha, columna_valor, pronostico_anios=10):\n",
        "    # Crear DataFrame vacío para almacenar los resultados\n",
        "    df_resultado = pd.DataFrame()\n",
        "\n",
        "    # Obtener lista de categorías únicas\n",
        "    categorias_unicas = data[columna_categoria].unique()\n",
        "\n",
        "    # Iterar sobre cada categoría\n",
        "    for categoria in categorias_unicas:\n",
        "        # Filtrar los datos para la categoría actual\n",
        "        df_categoria = data[data[columna_categoria] == categoria].copy()\n",
        "\n",
        "        # Convertir la columna de fecha a datetime (si no está en ese formato)\n",
        "        df_categoria[columna_fecha] = pd.to_datetime(df_categoria[columna_fecha])\n",
        "\n",
        "        # Establecer la columna de fecha como índice\n",
        "        df_categoria.set_index(columna_fecha, inplace=True)\n",
        "\n",
        "        # Ajustar el modelo ARIMA para la serie de tiempo actual\n",
        "        modelo_arima = ARIMA(df_categoria[columna_valor], order=(1, 1, 0))\n",
        "        resultado = modelo_arima.fit()\n",
        "\n",
        "        # Generar el pronóstico para los próximos 10 años (anual)\n",
        "        pronostico_anual = resultado.forecast(steps=pronostico_anios)\n",
        "\n",
        "        # Crear un rango de fechas para los próximos 10 años (anual)\n",
        "        fechas_pronostico = pd.date_range(df_categoria.index[-1], periods=pronostico_anios + 1, freq='Y')[1:]\n",
        "\n",
        "        # Crear un DataFrame para el pronóstico y completar los NaN\n",
        "        df_pronostico = pd.DataFrame({columna_fecha: fechas_pronostico, columna_valor: pronostico_anual})\n",
        "        df_pronostico.set_index(columna_fecha, inplace=True)\n",
        "\n",
        "        # Combinar el DataFrame de pronóstico con el DataFrame de la categoría actual\n",
        "        df_categoria = df_categoria.append(df_pronostico)\n",
        "\n",
        "        # Agregar la categoría al DataFrame de resultados\n",
        "        df_categoria[columna_categoria] = categoria\n",
        "        df_resultado = df_resultado.append(df_categoria)\n",
        "\n",
        "    return df_resultado"
      ],
      "metadata": {
        "id": "I3Ma5-y9tgeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def reemplazar_valores_negativos(df, categoria_col, valor_col):\n",
        "    # Crear una copia del DataFrame para no modificar el original\n",
        "    df_copia = df.copy()\n",
        "\n",
        "    # Agrupar los datos por categoría\n",
        "    grupos_categorias = df_copia.groupby(categoria_col)\n",
        "\n",
        "    # Recorrer cada grupo de categoría\n",
        "    for categoria, grupo in grupos_categorias:\n",
        "\n",
        "        if grupo.empty:\n",
        "            # Si el grupo está vacío, no hacer ninguna operación y continuar con el siguiente grupo\n",
        "            continue\n",
        "        # Obtener los valores de la serie de tiempo para la categoría actual\n",
        "        valores = grupo[valor_col].values\n",
        "\n",
        "         # Verificar si todos los valores en el grupo son cero\n",
        "        if np.all(valores == 0):\n",
        "            # Si todos son cero, no hacer ninguna operación y continuar con el siguiente grupo\n",
        "            continue\n",
        "\n",
        "        # Encontrar el menor número positivo presente en la serie\n",
        "        minimo_positivo = np.min(valores[valores > 0])\n",
        "\n",
        "        if minimo_positivo is np.nan:\n",
        "            minimo_positivo = np.min(valores)\n",
        "            minimo_positivo=minimo_positivo*-1\n",
        "\n",
        "        # Reemplazar los valores negativos con el menor número positivo\n",
        "        grupo.loc[grupo[valor_col] < 0, valor_col] = minimo_positivo\n",
        "\n",
        "        # Actualizar el grupo modificado en el DataFrame copia\n",
        "        df_copia.loc[grupo.index] = grupo\n",
        "\n",
        "    return df_copia"
      ],
      "metadata": {
        "id": "BcpPsozH3yGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def comparacion_con_anio_base(df, categoria_col, valor_col, anio_col, anio_base):\n",
        "    # Filtrar los datos para el año base\n",
        "    datos_anio_base = df[df[anio_col] == anio_base].copy()\n",
        "\n",
        "    # Renombrar la columna de valor para diferenciarla\n",
        "    datos_anio_base.rename(columns={valor_col: f'{valor_col}_({anio_base})'}, inplace=True)\n",
        "\n",
        "    # Merge para combinar los datos con el año base\n",
        "    df_con_base = pd.merge(df, datos_anio_base[[categoria_col, f'{valor_col}_({anio_base})']],\n",
        "                           on=categoria_col, how='left')\n",
        "\n",
        "    # Calcular la comparación para cada año con respecto al año base\n",
        "    for anio in df[anio_col].unique():\n",
        "        if anio != anio_base:\n",
        "            columna_comparacion = f'{valor_col}_({anio}-{anio_base})/{anio_base}'\n",
        "            df_con_base[columna_comparacion] = (df_con_base[valor_col] -\n",
        "                                                df_con_base[f'{valor_col}_({anio_base})']) / df_con_base[f'{valor_col}_({anio_base})']\n",
        "\n",
        "    return df_con_base"
      ],
      "metadata": {
        "id": "kAbhWOhU4IiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Funcion que agrega los valores anterios de valorización respecto a la base\n",
        "def agregar_valores_anteriores(df, categoria_col, valor_col, anios_anteriores=3):\n",
        "    # Crear una copia del DataFrame para no modificar el original\n",
        "    df_copia = df.copy()\n",
        "\n",
        "    # Ordenar el DataFrame por categoría y año para asegurar que los valores estén en orden\n",
        "    df_copia.sort_values([categoria_col, 'Year'], inplace=True)\n",
        "\n",
        "    # Crear columnas nuevas para cada año anterior\n",
        "    for i in range(1, anios_anteriores + 1):\n",
        "        nombre_columna = f\"{valor_col}_anterior_{i}\"\n",
        "        df_copia[nombre_columna] = df_copia.groupby(categoria_col)[valor_col].shift(i)\n",
        "\n",
        "    return df_copia"
      ],
      "metadata": {
        "id": "WM_TOQvVLxKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agregar_valores_futuros(df, categoria_col, valor_col, anios_futuros=3):\n",
        "    # Crear una copia del DataFrame para no modificar el original\n",
        "    df_copia = df.copy()\n",
        "\n",
        "    # Ordenar el DataFrame por categoría y año para asegurar que los valores estén en orden\n",
        "    df_copia.sort_values([categoria_col, 'Year'], inplace=True)\n",
        "\n",
        "    # Crear columnas nuevas para cada año futuro\n",
        "    for i in range(1, anios_futuros + 1):\n",
        "        nombre_columna = f\"{valor_col}_futuro_{i}\"\n",
        "        df_copia[nombre_columna] = df_copia.groupby(categoria_col)[valor_col].shift(-i)\n",
        "\n",
        "    return df_copia"
      ],
      "metadata": {
        "id": "HsMk5jwJ4Pjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def columnas_con_20_por_ciento_ceros_o_nan(df):\n",
        "    # Contar la cantidad de valores 0 o NaN en cada columna\n",
        "    valores_ceros_o_nan = df.isin([0, np.nan]).sum()\n",
        "\n",
        "    # Calcular el porcentaje de valores 0 o NaN en cada columna\n",
        "    porcentaje_ceros_o_nan = valores_ceros_o_nan / len(df) * 100\n",
        "\n",
        "    # Filtrar las columnas que tienen más del 60% de valores 0 o NaN\n",
        "    columnas_con_mas_del_60_por_ciento = porcentaje_ceros_o_nan[porcentaje_ceros_o_nan > 20].index.tolist()\n",
        "\n",
        "    return columnas_con_mas_del_60_por_ciento"
      ],
      "metadata": {
        "id": "xo6oLdQSR0B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizar_variable_entre_0_y_1(df, columna):\n",
        "    # Obtener el valor mínimo y máximo de la columna\n",
        "    min_valor = df[columna].min()\n",
        "    max_valor = df[columna].max()\n",
        "\n",
        "    # Aplicar la normalización min-max para que quede entre 0 y 1\n",
        "    df[columna] = (df[columna] - min_valor) / (max_valor - min_valor)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "hM9bRbtDwlMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_variables_limpieza(dataset):\n",
        "    # Eliminar valores infinitos o muy grandes\n",
        "    dataset = dataset.replace([np.inf, -np.inf], np.nan)\n",
        "    dataset = dataset.fillna(0)  # Reemplazar valores NaN por cero o cualquier otro valor apropiado\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    normalized_dataset = scaler.fit_transform(dataset)\n",
        "    normalized_dataset = pd.DataFrame(normalized_dataset, columns=dataset.columns)\n",
        "    return normalized_dataset"
      ],
      "metadata": {
        "id": "lE5CFuBHWlYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eliminar_outliers_iqr(df, variable_col):\n",
        "    Q1 = df[variable_col].quantile(0.25)\n",
        "    Q3 = df[variable_col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    df_sin_outliers = df[(df[variable_col] >= lower_bound) & (df[variable_col] <= upper_bound)]\n",
        "    return df_sin_outliers"
      ],
      "metadata": {
        "id": "A68AqXsXZiCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eliminar_registros_con_cero(df, columna):\n",
        "    # Filtrar las filas que contienen cero en la columna especificada\n",
        "    df_sin_cero = df[df[columna] != 0]\n",
        "\n",
        "    return df_sin_cero"
      ],
      "metadata": {
        "id": "7ZHJKVNWav7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "def efecto_por_categoria(df, columna_categoria, columna_numerica, poblacion, nivel_confianza=0.10):\n",
        "    \"\"\"\n",
        "    Determina si la diferencia de promedios para cada categoría es significativa en relación con la población,\n",
        "    la dirección del efecto y la magnitud del efecto.\n",
        "\n",
        "    Parámetros:\n",
        "    - df: DataFrame que contiene la columna de categoría y la columna numérica a analizar.\n",
        "    - columna_categoria: Nombre de la columna que contiene las categorías.\n",
        "    - columna_numerica: Nombre de la columna que contiene el valor numérico a analizar.\n",
        "    - poblacion: DataFrame que representa la población completa con la columna 'columna_numerica'.\n",
        "    - nivel_confianza: Nivel de confianza para realizar la prueba t (por defecto es 0.95).\n",
        "\n",
        "    Retorna:\n",
        "    - DataFrame que muestra el resultado del análisis, indicando si la diferencia de promedios\n",
        "      para cada categoría es significativa en relación con la población, la dirección del efecto\n",
        "      y la magnitud del efecto.\n",
        "    \"\"\"\n",
        "    resultados = []\n",
        "\n",
        "    media_poblacion = poblacion[columna_numerica].mean()\n",
        "    #print(media_poblacion)\n",
        "    magnitudes = {\n",
        "        'Muy Bajo': 5,\n",
        "        'Bajo': 10,\n",
        "        'Medio': 15,\n",
        "        'Alto': 20,\n",
        "        'Muy Alto':30\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    for categoria in df[columna_categoria].unique():\n",
        "        data_por_categoria = df[df[columna_categoria] == categoria][columna_numerica]\n",
        "        t_stat, p_value = ttest_ind(data_por_categoria, poblacion[columna_numerica])\n",
        "        #print(categoria)\n",
        "        #print(data_por_categoria.mean())\n",
        "        #print('p_value')\n",
        "        #print(p_value)\n",
        "        #print('t_stat')\n",
        "        #print(t_stat)\n",
        "        #print(data_por_categoria.mean() - media_poblacion)\n",
        "        #print((data_por_categoria.mean() - media_poblacion)*100/media_poblacion)\n",
        "\n",
        "        if p_value < (1 - nivel_confianza):\n",
        "            diferencia_promedios = data_por_categoria.mean() - media_poblacion\n",
        "\n",
        "            if diferencia_promedios > 0:\n",
        "                direccion = \"Positivo\"\n",
        "            elif diferencia_promedios < 0:\n",
        "                direccion = \"Negativo\"\n",
        "            else:\n",
        "                direccion = \"Indeterminado\"\n",
        "\n",
        "            Porcentaje_diferencia=abs(diferencia_promedios/media_poblacion)*100\n",
        "\n",
        "            for mag, limite in magnitudes.items():\n",
        "                if Porcentaje_diferencia <= limite:\n",
        "                    magnitud = mag\n",
        "                    break\n",
        "            else:\n",
        "                magnitud = \"Indeterminado\"\n",
        "\n",
        "            resultado = f\"{direccion} - {magnitud}\"\n",
        "        else:\n",
        "            resultado = \"No significativo\"\n",
        "\n",
        "        resultados.append(resultado)\n",
        "\n",
        "    resultados_df = pd.DataFrame(resultados, index=df[columna_categoria].unique(), columns=[columna_numerica])\n",
        "\n",
        "    return resultados_df"
      ],
      "metadata": {
        "id": "MODgQ5kkavsj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}