{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#pip install unidecode"
      ],
      "metadata": {
        "id": "PirKvJ8KLcTH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression, f_classif\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import warnings\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, r2_score, confusion_matrix\n",
        "from sklearn.utils import check_array\n",
        "from unidecode import unidecode\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n"
      ],
      "metadata": {
        "id": "AMVVyKt9Fsfl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def balancear_datos(X, Y):\n",
        "    # Instancia el RandomOverSampler\n",
        "    ros = RandomOverSampler(random_state=13)\n",
        "\n",
        "    # Aplica la técnica de sobremuestreo para balancear las clases\n",
        "    X_resampled, Y_resampled = ros.fit_resample(X, Y)\n",
        "\n",
        "    return X_resampled, Y_resampled\n",
        "print('balancear_datos Loaded')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1_FVaz1frvs",
        "outputId": "f14c1a70-94e8-4271-e02a-0c9540d1fb87"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "balancear_datos Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Qovp-AeFSMo",
        "outputId": "f345b4c3-5d7d-496b-d9a3-af1deaec3ce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selectkbest_features Loaded\n"
          ]
        }
      ],
      "source": [
        "# SelectKBest : Selecciona las variables Relevantes Utilizando SelectKBest\n",
        "\n",
        "def Selectkbest_features(X, y, k,tipo_modelo=f_regression):\n",
        "    selector = SelectKBest(score_func=f_regression, k=k)  # Utilizamos f_regression como función de puntuación\n",
        "    X_new = selector.fit_transform(X, y)  # Seleccionamos las k mejores características\n",
        "\n",
        "    mask = selector.get_support()  # Máscara booleana de las características seleccionadas\n",
        "    selected_features = [feature for feature, mask_value in zip(X.columns, mask) if mask_value]\n",
        "    selected_scores = selector.scores_[selector.get_support()]\n",
        "\n",
        "    # Crear un DataFrame con las variables seleccionadas y sus puntuaciones\n",
        "    selected_data = pd.DataFrame({'Variable': selected_features, 'Puntuacion': selected_scores})\n",
        "\n",
        "#f_regression; f_classif\n",
        "    return selected_data\n",
        "print('Selectkbest_features Loaded')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RFE: Fucnion que obtiene las variables utilizanfo el modelo de Recursive Feature Elimination.\n",
        "\n",
        "def rfe_features(estimator, X, y, n_features_to_select=None):\n",
        "\n",
        "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "    selector = RFE(estimator, n_features_to_select=n_features_to_select)\n",
        "    selector.fit(X, y)\n",
        "\n",
        "    # Obtener las variables seleccionadas\n",
        "    selected_features = X.columns[selector.support_].tolist()\n",
        "\n",
        "\n",
        "    # Obtener los puntajes de las características seleccionadas\n",
        "    feature_scores = selector.estimator_.coef_\n",
        "\n",
        "    # Crear el DataFrame de resultado\n",
        "    df_result = pd.DataFrame({'Variable': selected_features, 'Score': feature_scores})\n",
        "    warnings.filterwarnings(\"default\", category=DeprecationWarning)\n",
        "    return df_result\n",
        "print('rfe_features Loaded')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gox4ZVmcFnjD",
        "outputId": "1bcd5993-5bfc-42e8-e70d-2fd8808d6fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rfe_features Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Integración de Variables\n",
        "\n",
        "def integrate_features(Selectkbest_List,Boruta_List,Rfe_List):\n",
        "\n",
        "    integrated_features = list(set(Selectkbest_List + Boruta_List + Rfe_List))\n",
        "\n",
        "    return integrated_features\n",
        "print('Integración de Variables Loaded')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APIeOedPFngB",
        "outputId": "06853120-c3eb-4738-b4a5-40d1cefb8f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integración de Variables Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Funcion de evaluacion de los modelos\n",
        "\n",
        "def evaluate_results(y_true, y_pred):\n",
        "    # Curva ROC\n",
        "    roc_auc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "    # Error cuadrático medio (MSE)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    # Raíz del error cuadrático medio (RMSE)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # Coeficiente de determinación (R2)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # Error porcentual absoluto medio (MAPE)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "    return roc_auc, mse, rmse, r2, mape\n",
        "print('evaluacion de los modelos Loaded')"
      ],
      "metadata": {
        "id": "ewUJSBhTAeLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generacion de Componentes Principales\n",
        "\n",
        "def generar_componentes_principales_X(dataframe, n_componentes):\n",
        "    # Inicializar el objeto PCA con el número de componentes deseados\n",
        "    pca = PCA(n_components=n_componentes)\n",
        "\n",
        "    # Realizar el PCA y transformar los datos\n",
        "    componentes_principales = pca.fit_transform(dataframe)\n",
        "\n",
        "    # Crear un DataFrame con los componentes principales\n",
        "    df_componentes = pd.DataFrame(data=componentes_principales, columns=[f'Componente_{i+1}' for i in range(n_componentes)])\n",
        "\n",
        "    return df_componentes\n",
        "print('Pca_components Loaded')"
      ],
      "metadata": {
        "id": "NQ9RCVb3OyAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a9df94-9984-4c74-a755-a6d475aa41a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pca_components Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Funcion que encuentra las variables utilziando tensorflow\n",
        "\n",
        "def find_important_variables(X, y, model_type='random_forest', n_estimators=100):\n",
        "    # Configurar TensorFlow para utilizar la GPU\n",
        "    physical_devices = tf.config.list_physical_devices('GPU')\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "    if model_type == 'random_forest':\n",
        "        # Crear el modelo Random Forest\n",
        "        model = RandomForestRegressor(n_estimators=n_estimators)\n",
        "\n",
        "    elif model_type == 'xgboost':\n",
        "        # Crear el modelo XGBoost\n",
        "        model = XGBRegressor(n_estimators=n_estimators)\n",
        "\n",
        "    else:\n",
        "        raise ValueError('Model type not supported.')\n",
        "\n",
        "    # Convertir los datos a tensores de TensorFlow\n",
        "    X_tensor = tf.convert_to_tensor(X.values, dtype=tf.float32)\n",
        "    y_tensor = tf.convert_to_tensor(y.values, dtype=tf.float32)\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    with tf.device('/GPU:0'):\n",
        "        model.fit(X_tensor, y_tensor)\n",
        "\n",
        "    # Obtener la importancia de las variables\n",
        "    if model_type == 'random_forest':\n",
        "        feature_importances = model.feature_importances_\n",
        "    elif model_type == 'xgboost':\n",
        "        feature_importances = model.feature_importances_\n",
        "    else:\n",
        "        feature_importances = None\n",
        "\n",
        "    if feature_importances is not None:\n",
        "        # Obtener los nombres de las variables\n",
        "        variable_names = X.columns\n",
        "\n",
        "        # Crear un DataFrame con las variables y su importancia\n",
        "        importance_df = pd.DataFrame({'Variable': variable_names, 'Importance': feature_importances})\n",
        "\n",
        "        # Ordenar las variables por su importancia en orden descendente\n",
        "        importance_df = importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "        return importance_df\n",
        "    else:\n",
        "        raise ValueError('Feature importances not available.')\n",
        "print('Improtancia con XGBoost y Random forest Loaded')"
      ],
      "metadata": {
        "id": "N8YvHNoSAbNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modelo para encontrar variables importantes usando XGBOOST y RANDOM FOREST\n",
        "def find_important_variables_Clasificacion(X, y, model_type='xgboost', n_estimators=100):\n",
        "    if model_type == 'xgboost':\n",
        "        # Crear el modelo XGBoost clasificador\n",
        "        model = xgb.XGBClassifier(n_estimators=n_estimators)\n",
        "\n",
        "    elif model_type == 'random_forest':\n",
        "        # Crear el modelo Random Forest clasificador\n",
        "        model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
        "\n",
        "    else:\n",
        "        raise ValueError('Model type not supported. Use \"xgboost\" or \"random_forest\".')\n",
        "\n",
        "\n",
        "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Obtener la importancia de las variables\n",
        "    feature_importances = model.feature_importances_\n",
        "\n",
        "    # Obtener los nombres de las variables\n",
        "    variable_names = X.columns\n",
        "\n",
        "    # Crear un DataFrame con las variables y su importancia\n",
        "    importance_df = pd.DataFrame({'Variable': variable_names, 'Importance': feature_importances})\n",
        "\n",
        "    # Ordenar las variables por su importancia en orden descendente\n",
        "    importance_df = importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    return importance_df"
      ],
      "metadata": {
        "id": "FZocdMbH84l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xgboost_model_evaluation(X_train, X_test, y_train, y_test):\n",
        "    # Definir los hiperparámetros a ajustar\n",
        "    param_grid = {\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.1, 0.01, 0.001],\n",
        "        'n_estimators': [100, 500, 1000]\n",
        "    }\n",
        "\n",
        "    # Crear el modelo XGBoost\n",
        "    xgb_model = XGBRegressor()\n",
        "\n",
        "    # Realizar Grid Search con validación cruzada\n",
        "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Obtener los mejores hiperparámetros\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Crear el modelo XGBoost con los mejores hiperparámetros\n",
        "    xgb_best_model = XGBRegressor(**best_params)\n",
        "\n",
        "    # Ajustar el modelo con los mejores hiperparámetros a los datos de entrenamiento\n",
        "    xgb_best_model.fit(X_train, y_train)\n",
        "\n",
        "    # Realizar la predicción en el grupo de prueba\n",
        "    y_pred = xgb_best_model.predict(X_test)\n",
        "\n",
        "    # Calcular el error cuadrático medio (MSE) del modelo en el grupo de prueba\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    # Calcular el coeficiente de determinación (R2) del modelo en el grupo de prueba\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Calcular el error porcentual absoluto medio (MAPE) del modelo en el grupo de prueba\n",
        "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
        "\n",
        "    return y_pred, best_params, mse, r2, mape\n",
        "print('XG_BOOST Regression Loaded')"
      ],
      "metadata": {
        "id": "GogvNWQB9kVU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e83780-b311-4d27-ccb7-f82f9d266b4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XG_BOOST Regression Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def random_forest_model_evaluation(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Genera la predicción y evaluación de un modelo RandomForestRegressor utilizando Grid Search para encontrar los mejores hiperparámetros.\n",
        "\n",
        "    Parámetros:\n",
        "    - X_train: matriz de características para el grupo de entrenamiento.\n",
        "    - X_test: matriz de características para el grupo de prueba.\n",
        "    - y_train: vector de etiquetas para el grupo de entrenamiento.\n",
        "    - y_test: vector de etiquetas para el grupo de prueba.\n",
        "\n",
        "    Retorna:\n",
        "    - y_pred: predicciones del modelo en el grupo de prueba.\n",
        "    - best_params: mejores hiperparámetros encontrados mediante Grid Search.\n",
        "    - accuracy: exactitud (accuracy) del modelo en el grupo de prueba.\n",
        "    \"\"\"\n",
        "    # Definir los hiperparámetros a ajustar\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'max_depth': [None, 5, 10],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "\n",
        "    # Crear el modelo RandomForestRegressor\n",
        "    rf_model = RandomForestRegressor()\n",
        "\n",
        "    # Realizar Grid Search con validación cruzada\n",
        "    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, scoring='accuracy', cv=5)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Obtener los mejores hiperparámetros\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Crear el modelo RandomForestRegressor con los mejores hiperparámetros\n",
        "    rf_best_model = RandomForestRegressor(**best_params)\n",
        "\n",
        "    # Ajustar el modelo con los mejores hiperparámetros a los datos de entrenamiento\n",
        "    rf_best_model.fit(X_train, y_train)\n",
        "\n",
        "    # Realizar la predicción en el grupo de prueba\n",
        "    y_pred = rf_best_model.predict(X_test)\n",
        "\n",
        "    # Calcular la exactitud del modelo en el grupo de prueba\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return y_pred, best_params, accuracy\n",
        "print('Random Forest Loaded')"
      ],
      "metadata": {
        "id": "toHP5vZk-wN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c78a85d-9979-4d77-b903-c0e639c5f2a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Función que elimina las tildes y diacríticos de un texto.\n",
        "def quitar_tildes(texto):\n",
        "    texto_sin_tildes = unidecode(texto)\n",
        "    return texto_sin_tildes\n",
        "print('quitar_tildes Loaded')"
      ],
      "metadata": {
        "id": "jgZaT4XxU0vc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c46dda-e3dc-4418-b941-2e46042a35a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quitar_tildes Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Función completa datos faltantes de serie de timepo\n",
        "def completar_datos_serie_tiempo(df,inicio,fin):\n",
        "\n",
        "    # Convert 'Year' to datetime type with date set to December 31st\n",
        "    df['Year'] = pd.to_datetime(df['Year'].astype(str) + '-12-31')\n",
        "\n",
        "    # Set 'Year' and 'Category' as the index\n",
        "    df.set_index(['Year', 'Category'], inplace=True)\n",
        "\n",
        "    # Create a new DataFrame with all years and categories from the first year to the last year in the available data\n",
        "    start_year = inicio\n",
        "    end_year = fin\n",
        "    all_years = pd.DataFrame(index=pd.MultiIndex.from_product([[str(y) + '-12-31' for y in range(start_year, end_year+1)], df.index.get_level_values('Category').unique()], names=['Year', 'Category']))\n",
        "\n",
        "\n",
        "    all_years['join']=all_years.index.get_level_values('Year').astype(str) + '-' + all_years.index.get_level_values('Category').astype(str)\n",
        "    df['join']=df.index.get_level_values('Year').astype(str) + '-' + df.index.get_level_values('Category').astype(str)\n",
        "\n",
        "\n",
        "    # Join the existing DataFrame with all_years\n",
        "    all_years.reset_index(inplace=True)\n",
        "    all_years['Year'] = pd.to_datetime(all_years['Year'])\n",
        "    df_complete = pd.merge(all_years, df,on='join',how='left')\n",
        "    #df_complete_s=df_complete\n",
        "    df_complete.set_index(['Year', 'Category'], inplace=True)\n",
        "    df_complete = df_complete.drop('join', axis=1)\n",
        "\n",
        "\n",
        "    # Obtener la lista de columnas que necesitan ser completadas\n",
        "    columns_to_interpolate = df_complete.columns.tolist()\n",
        "\n",
        "\n",
        "    for column in columns_to_interpolate:\n",
        "        df_complete[column] = df_complete.groupby('Category',group_keys=False)[column].apply(lambda x: x.interpolate(limit_direction='both'))\n",
        "\n",
        "    df_complete.reset_index(inplace=True)\n",
        "    df_complete=df_complete.fillna(0)\n",
        "    df_complete['Year'] = df_complete['Year'].dt.year\n",
        "\n",
        "\n",
        "    return df_complete\n",
        "print('completar_datos_serie_tiempo Loaded')"
      ],
      "metadata": {
        "id": "qf9sSh0E1Lnp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac8fc56-6712-4063-f5d6-26d87a2518f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "completar_datos_serie_tiempo Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#elimina las filas con NAN\n",
        "def eliminar_filas_nan(dataframe, columna):\n",
        "    dataframe_sin_nan = dataframe.dropna(subset=[columna], inplace=False)\n",
        "    return dataframe_sin_nan\n",
        "print('eliminar_filas_nan Loaded')"
      ],
      "metadata": {
        "id": "_xF-IntM4AI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14448dab-b9b7-458b-af81-31677b42650b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eliminar_filas_nan Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#si encuentra numeros menores los convierte en Nan\n",
        "def cambiar_numeros_menores(dataframe, valor, columna_excluida):\n",
        "    for columna in dataframe.columns:\n",
        "        if columna != columna_excluida:\n",
        "            dataframe[columna] = np.where(dataframe[columna] < valor, np.nan, dataframe[columna])\n",
        "    return dataframe\n",
        "print('cambiar_numeros_menores Loaded')"
      ],
      "metadata": {
        "id": "I2ddE96T6M1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4b78f6b-56a9-49fc-91ef-84b78569827e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cambiar_numeros_menores Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Funcion que Normaliza variables numericas\n",
        "\n",
        "def normalize_variables(dataset):\n",
        "    scaler = MinMaxScaler()\n",
        "    normalized_dataset = scaler.fit_transform(dataset)\n",
        "    normalized_dataset = pd.DataFrame(normalized_dataset, columns=dataset.columns)\n",
        "    return normalized_dataset\n",
        "print('normalize_variables Loaded')"
      ],
      "metadata": {
        "id": "2cw4m53od6iy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e281a9-e096-4332-8f88-ec474e8a017c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normalize_variables Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calcula el crecimiento respecto a un año especifico\n",
        "def calcular_crecimiento_ingresos(data, years_offset,Columna,ingresos,Yearp):\n",
        "    data_sorted = data.sort_values(by=[Yearp])\n",
        "    data_pivot = data_sorted.pivot(index=Columna, columns=Yearp, values=ingresos)\n",
        "\n",
        "    # Obtener todas las columnas de años en el DataFrame\n",
        "    years = data_pivot.columns.tolist()\n",
        "\n",
        "    crecimiento_data = []\n",
        "\n",
        "    if years_offset < 0:\n",
        "        years_offset = abs(years_offset)\n",
        "        for i in range(years_offset, len(years)):\n",
        "            current_year = years[i]\n",
        "            past_year = years[i - years_offset]\n",
        "\n",
        "            crecimiento = (data_pivot[current_year] - data_pivot[past_year]) / data_pivot[past_year]\n",
        "\n",
        "            crecimiento_data.append(pd.DataFrame({Columna: data_pivot.index, Yearp: current_year, 'Crecimiento': crecimiento}))\n",
        "    else:\n",
        "        for i in range(len(years) - years_offset):\n",
        "            current_year = years[i]\n",
        "            future_year = years[i + years_offset]\n",
        "\n",
        "            crecimiento = (data_pivot[future_year] - data_pivot[current_year]) / data_pivot[current_year]\n",
        "\n",
        "            crecimiento_data.append(pd.DataFrame({Columna: data_pivot.index, Yearp: current_year, 'Crecimiento': crecimiento}))\n",
        "\n",
        "    result = pd.concat(crecimiento_data, ignore_index=True)\n",
        "\n",
        "    return result\n",
        "print('calcular_crecimiento_ingresos Loaded')"
      ],
      "metadata": {
        "id": "J126QWVdK-D5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b897e10b-16dd-4a96-f807-b89a251e5886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calcular_crecimiento_ingresos Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Genera grupos de variables que comparten alta correlacion\n",
        "def obtener_variables_correlacionadas(data, umbral):\n",
        "    # Calcular la matriz de correlación\n",
        "    matriz_correlacion = data.corr().abs()\n",
        "\n",
        "    # Crear un diccionario para mapear las variables con sus grupos\n",
        "    grupos = {}\n",
        "    num_grupo = 1\n",
        "\n",
        "    # Recorrer las variables y asignarles el número de grupo\n",
        "    for variable in matriz_correlacion.columns:\n",
        "        if variable not in grupos:\n",
        "            variables_correlacionadas = matriz_correlacion.index[matriz_correlacion[variable] >= umbral]\n",
        "            for var in variables_correlacionadas:\n",
        "                grupos[var] = num_grupo\n",
        "            num_grupo += 1\n",
        "\n",
        "    # Crear un DataFrame con las variables agrupadas\n",
        "    df_variables_correlacionadas = pd.DataFrame({'Variable': list(grupos.keys()), 'Grupo': list(grupos.values())})\n",
        "\n",
        "    return df_variables_correlacionadas\n",
        "print('obtener_variables_correlacionadas Loaded')"
      ],
      "metadata": {
        "id": "2tf02cgYltoh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51ff1e0d-8688-4590-a22d-9fa913f390ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "obtener_variables_correlacionadas Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generar_componentes_principales(df_datos, df_grupos):\n",
        "    grupos = df_grupos['Grupo'].unique()  # Obtener la lista de grupos únicos\n",
        "    componentes_p = pd.DataFrame()\n",
        "    for grupo in grupos:\n",
        "        # Filtrar las variables correspondientes al grupo actual\n",
        "        variables_grupo = df_grupos[df_grupos['Grupo'] == grupo]['Variable'].tolist()\n",
        "\n",
        "        # Filtrar las columnas del DataFrame de datos según las variables del grupo actual\n",
        "        datos_grupo = df_datos[variables_grupo]\n",
        "\n",
        "        # Aplicar PCA al DataFrame del grupo actual\n",
        "        pca = PCA(n_components=1)\n",
        "        componente_principal = pca.fit_transform(datos_grupo)\n",
        "\n",
        "        # Asignar nombre al componente principal\n",
        "        nombre_componente = \"Componente_Principal_\" + str(grupo)\n",
        "        df_temp = pd.DataFrame(componente_principal, columns=[nombre_componente])\n",
        "        componentes_p = pd.concat([componentes_p, df_temp], axis=1)\n",
        "        # Agregar el componente principal al DataFrame de datos\n",
        "\n",
        "    return componentes_p\n",
        "print('generar_componentes_principales Loaded')"
      ],
      "metadata": {
        "id": "tao3eNYRP4Dw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b471c8-acc3-4e15-9e59-97e9b03d315d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generar_componentes_principales Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RFE: Fucnion que obtiene las variables utilizanfo el modelo de Recursive Feature Elimination.\n",
        "\n",
        "def rfe_with_elasticnet(X, y, n_features_to_select):\n",
        "    # Crear el estimador ElasticNet\n",
        "    estimator = ElasticNet()\n",
        "\n",
        "    # Crear el objeto RFE\n",
        "    rfe = RFE(estimator=estimator, n_features_to_select=n_features_to_select)\n",
        "\n",
        "    # Realizar la selección de características\n",
        "    rfe.fit(X, y)\n",
        "\n",
        "    # Obtener los nombres de las características\n",
        "    feature_names = X.columns\n",
        "\n",
        "    # Obtener los coeficientes de importancia de las características\n",
        "    feature_importances = rfe.estimator_.coef_\n",
        "\n",
        "    # Crear un DataFrame con las características y sus coeficientes de importancia\n",
        "    result_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "\n",
        "    return result_df\n",
        "print('rfe_with_elasticnet Loaded')"
      ],
      "metadata": {
        "id": "5ySQGq29FnU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funcion para calcular RFE En classificación\n",
        "\n",
        "def select_features_with_rfe_Clasisification(X, y, num_features):\n",
        "    # Instanciar el modelo de clasificación (por ejemplo, Regresión Logística)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "    # Crear el objeto RFE y especificar el modelo y el número de características deseado\n",
        "    rfe = RFE(model, n_features_to_select=num_features)\n",
        "\n",
        "    # Ajustar RFE a los datos\n",
        "    rfe.fit(X, y)\n",
        "\n",
        "    # Obtener las características seleccionadas con sus coeficientes de importancia\n",
        "    selected_features = X.columns[rfe.support_]\n",
        "    coef_importance = rfe.estimator_.coef_[0]\n",
        "\n",
        "    # Crear un DataFrame con las características seleccionadas y sus coeficientes de importancia\n",
        "    result_df = pd.DataFrame({'Variable': selected_features, 'Importance': coef_importance})\n",
        "\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "epsNE8xi4xQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calcular_crecimiento_promedio_por_categoria_Predial(df, Year_inicio_periodo1, Year_fin_periodo1, Year_inicio_periodo2, Year_fin_periodo2):\n",
        "    # Filtrar el DataFrame por los años de cada periodo\n",
        "    periodo1 = df[(df['Año'] >= Year_inicio_periodo1) & (df['Año'] <= Year_fin_periodo1)]\n",
        "    periodo2 = df[(df['Año'] >= Year_inicio_periodo2) & (df['Año'] <= Year_fin_periodo2)]\n",
        "\n",
        "    # Calcular el promedio de valores para cada categoría en cada periodo\n",
        "    promedio_periodo1 = periodo1.groupby('CODIGO_DANE')['Predial'].mean()\n",
        "    promedio_periodo2 = periodo2.groupby('CODIGO_DANE')['Predial'].mean()\n",
        "\n",
        "    # Calcular el crecimiento porcentual promedio para cada categoría\n",
        "    crecimiento_promedio = ((promedio_periodo2 - promedio_periodo1) / promedio_periodo1) * 100\n",
        "\n",
        "    return crecimiento_promedio\n",
        "print('calcular_crecimiento_promedio_por_categoria_Predial Loaded')"
      ],
      "metadata": {
        "id": "_uLt92HqFnIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categorizar_por_boxplot(df,columna_valores):\n",
        "    # Calcular el primer, segundo y tercer cuartil\n",
        "    primer_cuartil = columna_valores.quantile(0.25)\n",
        "    segundo_cuartil = columna_valores.quantile(0.5)\n",
        "    tercer_cuartil = columna_valores.quantile(0.75)\n",
        "\n",
        "    # Calcular los límites del bigote inferior y superior\n",
        "    iqr = tercer_cuartil - primer_cuartil\n",
        "    limite_inferior = primer_cuartil - 1.5 * iqr\n",
        "    limite_superior = tercer_cuartil + 1.5 * iqr\n",
        "\n",
        "    # Categorizar los valores según los análisis de boxplot\n",
        "    categorias = []\n",
        "    for valor in columna_valores:\n",
        "        if valor < limite_inferior:\n",
        "            categorias.append('Muy baja')\n",
        "        elif valor < primer_cuartil:\n",
        "            categorias.append('Baja')\n",
        "        elif valor < tercer_cuartil:\n",
        "            categorias.append('Media')\n",
        "        elif valor <= limite_superior:\n",
        "            categorias.append('Alta')\n",
        "        else:\n",
        "            categorias.append('Muy alta')\n",
        "    df['recomendacion'] = categorias\n",
        "    return categorias\n",
        "print('categorizar_por_boxplot Loaded')"
      ],
      "metadata": {
        "id": "If8z7nT_LEcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calcular_crecimiento_promedio_por_categoria(df, columnas_procesar, Year_inicio_periodo1, Year_fin_periodo1, Year_inicio_periodo2, Year_fin_periodo2):\n",
        "    # Filtrar el DataFrame por los años de cada periodo\n",
        "    periodo1 = df[(df['Year'] >= Year_inicio_periodo1) & (df['Year'] <= Year_fin_periodo1)]\n",
        "    periodo2 = df[(df['Year'] >= Year_inicio_periodo2) & (df['Year'] <= Year_fin_periodo2)]\n",
        "\n",
        "    # Crear un DataFrame vacío para almacenar los resultados\n",
        "    resultados = pd.DataFrame()\n",
        "\n",
        "    # Iterar sobre las columnas y calcular el crecimiento por categoría\n",
        "    for columna in columnas_procesar:\n",
        "        # Calcular el promedio de valores para cada categoría en cada periodo\n",
        "        promedio_periodo1 = periodo1.groupby('Municipio')[columna].mean()\n",
        "        promedio_periodo2 = periodo2.groupby('Municipio')[columna].mean()\n",
        "\n",
        "        # Calcular el crecimiento porcentual promedio para cada categoría\n",
        "        crecimiento_promedio = ((promedio_periodo2 - promedio_periodo1) / promedio_periodo1) * 100\n",
        "\n",
        "        # Agregar el resultado al DataFrame resultados\n",
        "        resultados = pd.concat([resultados, crecimiento_promedio], axis=1)\n",
        "\n",
        "    # Renombrar las columnas con los nombres originales\n",
        "    resultados.columns = columnas_procesar\n",
        "\n",
        "    return resultados\n",
        "print('calcular_crecimiento_promedio_por_categoria Loaded')"
      ],
      "metadata": {
        "id": "gmgfBgDmcvSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def limitar_atipicos(df, columnas_procesar):\n",
        "    # Inicializar un DataFrame para almacenar los resultados\n",
        "    resultados = df.copy()\n",
        "\n",
        "    # Iterar sobre las columnas a procesar\n",
        "    for columna in columnas_procesar:\n",
        "        # Calcular el rango intercuartílico (IQR)\n",
        "        q1 = df[columna].quantile(0.25)\n",
        "        q3 = df[columna].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "\n",
        "        # Calcular los límites del bigote superior e inferior\n",
        "        upper_limit = q3 + 1.5 * iqr\n",
        "        lower_limit = q1 - 1.5 * iqr\n",
        "\n",
        "        # Reemplazar los valores atípicos con los límites del bigote\n",
        "        resultados[columna] = resultados[columna].apply(lambda x: upper_limit if x > upper_limit else lower_limit if x < lower_limit else x)\n",
        "\n",
        "    return resultados\n",
        "print('limitar_atipicos Loaded')"
      ],
      "metadata": {
        "id": "IS7-CU7AcwQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_dummies(df, columns_to_dummy):\n",
        "    for col in columns_to_dummy:\n",
        "        dummies = pd.get_dummies(df[col], prefix=col)\n",
        "        df = pd.concat([df, dummies], axis=1)\n",
        "        df.drop(col, axis=1, inplace=True)\n",
        "    return df\n",
        "print('crear_dummies Loaded')"
      ],
      "metadata": {
        "id": "EGoW3QaFeyDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Funcion para comvertir un data frame con años en columasn en filas y generar comparacion de fechas\n",
        "def year_columnas(Industria, palabra,ID,Year_inicio_periodo1, Year_fin_periodo1, Year_inicio_periodo2, Year_fin_periodo2):\n",
        "\n",
        "    new_columns = [col.replace(palabra, '') for col in Industria.columns] # quita un valor de una columna\n",
        "    Industria.rename(columns=dict(zip(Industria.columns, new_columns)), inplace=True) # quita un valor de una columna\n",
        "\n",
        "    Industria = Industria.melt(id_vars=[ID], var_name='Year', value_name='Valor') #Transpone los años en filas\n",
        "\n",
        "    Industria.rename(columns={ID: 'Category'}, inplace=True) #Cambio de nombre de columna\n",
        "    Industria=completar_datos_serie_tiempo(Industria,2008,2020) #Completa faltantes con interpolación\n",
        "\n",
        "    Industria.rename(columns={'Category': 'Municipio'}, inplace=True) #Cambio de nombre de columna\n",
        "    Industria.rename(columns={'Valor': 'Industria'}, inplace=True) #Cambio de nombre de columna\n",
        "    lista_columnas = Industria[Industria.columns.difference(['Municipio','Year'])].columns.tolist()\n",
        "    Industria=calcular_crecimiento_promedio_por_categoria(Industria,lista_columnas,Year_inicio_periodo1, Year_fin_periodo1, Year_inicio_periodo2, Year_fin_periodo2)\n",
        "    Industria=limitar_atipicos(Industria, lista_columnas)\n",
        "    Industria = Industria.fillna(0)\n",
        "    Industria=Industria.reset_index()\n",
        "    Industria.rename(columns={'index': 'Municipio'}, inplace=True) #Cambio de nombre de columna\n",
        "    return Industria"
      ],
      "metadata": {
        "id": "lFMDSN1S66bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(X_train, X_test, y_train, y_test, Modelo, param_grid=None, random_state=13):\n",
        "    if Modelo not in ['xgboost', 'random_forest']:\n",
        "        raise ValueError('Modelo debe ser \"xgboost\" o \"random_forest\".')\n",
        "\n",
        "    if Modelo == 'xgboost':\n",
        "        model = xgb.XGBClassifier(tree_method='gpu_hist', random_state=random_state)\n",
        "        model = xgb.XGBClassifier(random_state=random_state)\n",
        "    else:\n",
        "        model = RandomForestClassifier(random_state=random_state)\n",
        "\n",
        "    if param_grid is not None:\n",
        "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='roc_auc_ovr', cv=5, n_jobs=-1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        best_params = grid_search.best_params_\n",
        "        model.set_params(**best_params)\n",
        "\n",
        "    # Validación cruzada (k-fold cross-validation)\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc_ovr', n_jobs=-1)\n",
        "    print(\"Mean CV ROC AUC:\", cv_scores.mean())\n",
        "\n",
        "    # Curva de aprendizaje\n",
        "    #train_sizes, train_scores, test_scores = learning_curve(model, X_train, y_train, cv=5, scoring='roc_auc_ovr', n_jobs=-1)\n",
        "    train_sizes, train_scores, test_scores = learning_curve(model, X_train, y_train, cv=5, scoring='roc_auc_ovr', n_jobs=-1)\n",
        "    plt.figure()\n",
        "    plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', label='Training Score')\n",
        "    plt.plot(train_sizes, test_scores.mean(axis=1), 'o-', label='Test Score')\n",
        "    plt.xlabel('Training examples')\n",
        "    plt.ylabel('ROC AUC')\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    with tf.device('/GPU:0'):\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr')\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    return model, accuracy, roc_auc, conf_matrix, best_params"
      ],
      "metadata": {
        "id": "0CeIsOx81ANE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_eval, y_eval):\n",
        "    # Obtener las predicciones del modelo\n",
        "    y_pred = model.predict(X_eval)\n",
        "\n",
        "    # Calcular el accuracy\n",
        "    accuracy = accuracy_score(y_eval, y_pred)\n",
        "\n",
        "    # Calcular el valor de la curva ROC\n",
        "    #roc_auc = roc_auc_score(y_eval, y_pred,multi_class='ovr')\n",
        "    roc_auc = roc_auc_score(y_eval, model.predict_proba(X_eval), multi_class='ovr')\n",
        "    # Calcular la matriz de confusión\n",
        "    conf_matrix = confusion_matrix(y_eval, y_pred)\n",
        "\n",
        "    # Crear el DataFrame con los resultados\n",
        "    result_df = pd.DataFrame({\n",
        "        'Accuracy': [accuracy],\n",
        "        'ROC_AUC': [roc_auc]\n",
        "    })\n",
        "\n",
        "    return result_df,conf_matrix"
      ],
      "metadata": {
        "id": "TJNFW6h31XZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def obtener_clasificacion_y_scores(modelo, X, lista_id):\n",
        "    \"\"\"\n",
        "    Obtiene la clasificación de y y el score de cada categoría para un conjunto de datos X.\n",
        "\n",
        "    Parámetros:\n",
        "        modelo: Modelo entrenado capaz de realizar predicciones multiclase.\n",
        "        X: Conjunto de datos de entrada para realizar las predicciones.\n",
        "        lista_id (list): Lista de identificadores (ID) correspondientes a los datos en X.\n",
        "\n",
        "    Retorna:\n",
        "        pandas DataFrame: Un DataFrame con las columnas ID, y, y las scores para cada categoría.\n",
        "    \"\"\"\n",
        "    # Realizar predicciones para el conjunto de datos X\n",
        "    y_pred_prob = modelo.predict_proba(X)\n",
        "\n",
        "    # Obtener las categorías del modelo\n",
        "    categorias = modelo.classes_\n",
        "\n",
        "    # Crear una lista con el resultado para cada fila\n",
        "    resultados = []\n",
        "    for i in range(len(X)):\n",
        "        fila = [lista_id[i]] + [categorias[np.argmax(y_pred_prob[i])]] + y_pred_prob[i].tolist()\n",
        "        resultados.append(fila)\n",
        "\n",
        "    # Crear el DataFrame con los resultados\n",
        "    columnas = ['ID', 'y'] + [f'Score_{categoria}' for categoria in categorias]\n",
        "    df_resultados = pd.DataFrame(resultados, columns=columnas)\n",
        "\n",
        "    return df_resultados"
      ],
      "metadata": {
        "id": "o3FpTWLy8VwH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}